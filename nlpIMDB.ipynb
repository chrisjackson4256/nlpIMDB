{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle IMDB Review Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data and check it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header = 0, \n",
    "                    delimiter=\"\\t\", quoting = 3)\n",
    "\n",
    "# shape of the dataframe\n",
    "#print train.shape\n",
    "\n",
    "# column name\n",
    "#print train.columns.values\n",
    "\n",
    "# first few rows\n",
    "#print train.head(3)\n",
    "\n",
    "# check out a review\n",
    "#print train[\"sentiment\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 reviews processed.\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    ''' function to convert raw IMDB review \n",
    "        to list of words'''\n",
    "    # remove markup and tags\n",
    "    bs_review = BeautifulSoup( raw_review )\n",
    "    # remove numbers and punctuation\n",
    "    letters_only = re.sub(r'[^a-zA-Z]', ' ', bs_review.get_text())\n",
    "    # convert to lower case\n",
    "    lower_case = letters_only.lower()\n",
    "    # split string into list\n",
    "    words_only = lower_case.split()\n",
    "    # define the stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # remove stop words from review\n",
    "    words = [w for w in words_only if w not in stops]\n",
    "    # stem the words\n",
    "    stem_words = [stemmer.stem(w) for w in words]\n",
    "    # lemmatize the words\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    return \" \".join(stem_words)\n",
    "\n",
    "clean_train_reviews = []\n",
    "for i, rev in enumerate(train[\"review\"]):\n",
    "    if (i + 1) % 5000 == 0:\n",
    "        print \"{} reviews processed.\".format(i+1)\n",
    "    clean_train_reviews.append( review_to_words( rev ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Features from a Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, we need to convert the reviews to some kind of numeric \n",
    "# representation for machine learning.  To do this, we'll use the \n",
    "# 'Bag of Words' approach.\n",
    "\n",
    "# The 'Bag of Words' model learns a vocabulary from all of the \n",
    "# documents, then models each document by counting the number of\n",
    "# times each word appears.\n",
    "\n",
    "# We take the full text and form a feature vector that contains the \n",
    "# full 'vocabulary'.  Then, for each review, we count how many times \n",
    "# each word appears.\n",
    "\n",
    "# Using the \"feature_extraction\" module from scikit-learn:\n",
    "\n",
    "# first, initialize the \"CountVectorizer\" object, which is scikit-\n",
    "# learn's bag of words tool:\n",
    "vectorizer = CountVectorizer(max_features = 5000)\n",
    "\n",
    "# fit_transform learns the vocabulary dictionary and returns the \n",
    "# term-document matrix\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# convert to an array since they're easier to work with\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "#print vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data_features, \n",
    "                                                    train['sentiment'],\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest = forest.fit(X_train, y_train)\n",
    "\n",
    "pred = forest.predict(X_test)\n",
    "\n",
    "print \"accuracy = \", accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg = logreg.fit(X_train, y_train)\n",
    "\n",
    "print \"accuracy = \", accuracy_score(y_test, logreg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a Support Vector Machine (runs SLOW!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#svmclass = SVC()\n",
    "#svmclass = svmclass.fit(X_train, y_train)\n",
    "\n",
    "#print \"accuracy = \", accuracy_score(y_test, svmclass.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Prediction and Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the test data\n",
    "test = pd.read_csv(\"data/testData.tsv\", header = 0, \n",
    "                   delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# Clean the test reviews\n",
    "clean_test_reviews = []\n",
    "for i, rev in enumerate(test[\"review\"]):\n",
    "    if (i + 1) % 5000 == 0:\n",
    "        print \"{} reviews processed.\".format(i+1)\n",
    "    clean_test_reviews.append( review_to_words( rev ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bag of words for the test set\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "# convert to an array since they're easier to work with\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the Random Forest to make predictions for sentiment\n",
    "# from test reviews\n",
    "rf_pred = forest.predict(test_data_features)\n",
    "\n",
    "# Use logistic regression to make predictions for sentiment\n",
    "# from test reviews\n",
    "lr_pred = logreg.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy results to a pandas DataFrame\n",
    "resultsRF = pd.DataFrame( data={\"id\": test[\"id\"], \"sentiment\": rf_pred} )\n",
    "resultsLR = pd.DataFrame( data={\"id\": test[\"id\"], \"sentiment\": lr_pred} )\n",
    "\n",
    "# Use pandas to output results to csv file\n",
    "resultsRF.to_csv(\"data/BagOfWords_RandomForest.csv\", index=False, quoting=3)\n",
    "resultsLR.to_csv(\"data/BagOfWords_LogReg.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
